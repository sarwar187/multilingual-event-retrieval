{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python36964bitpytorchp36conda89fe1e12953f4cfda56e52ce258dc1db",
   "display_name": "Python 3.6.9 64-bit ('pytorch_p36': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "2020-02-16 10:09:35 - Load pretrained SentenceTransformer: bert-base-nli-mean-tokens\n2020-02-16 10:09:35 - Did not find a / or \\ in the name. Assume to download model from server\n2020-02-16 10:09:35 - Load SentenceTransformer from folder: /net/home/smsarwar/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip\n2020-02-16 10:09:35 - loading configuration file /net/home/smsarwar/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip/0_BERT/config.json\n2020-02-16 10:09:35 - Model config {\n  \"attention_probs_dropout_prob\": 0.1,\n  \"bos_token_id\": 0,\n  \"do_sample\": false,\n  \"eos_token_ids\": 0,\n  \"finetuning_task\": null,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\"\n  },\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"is_decoder\": false,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1\n  },\n  \"layer_norm_eps\": 1e-12,\n  \"length_penalty\": 1.0,\n  \"max_length\": 20,\n  \"max_position_embeddings\": 512,\n  \"num_attention_heads\": 12,\n  \"num_beams\": 1,\n  \"num_hidden_layers\": 12,\n  \"num_labels\": 2,\n  \"num_return_sequences\": 1,\n  \"output_attentions\": false,\n  \"output_hidden_states\": false,\n  \"output_past\": true,\n  \"pad_token_id\": 0,\n  \"pruned_heads\": {},\n  \"repetition_penalty\": 1.0,\n  \"temperature\": 1.0,\n  \"top_k\": 50,\n  \"top_p\": 1.0,\n  \"torchscript\": false,\n  \"type_vocab_size\": 2,\n  \"use_bfloat16\": false,\n  \"vocab_size\": 30522\n}\n\n2020-02-16 10:09:35 - loading weights file /net/home/smsarwar/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip/0_BERT/pytorch_model.bin\n2020-02-16 10:09:37 - Model name '/net/home/smsarwar/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip/0_BERT' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1). Assuming '/net/home/smsarwar/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip/0_BERT' is a path or url to a directory containing tokenizer files.\n2020-02-16 10:09:37 - Didn't find file /net/home/smsarwar/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip/0_BERT/tokenizer_config.json. We won't load it.\n2020-02-16 10:09:37 - loading file /net/home/smsarwar/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip/0_BERT/vocab.txt\n2020-02-16 10:09:37 - loading file /net/home/smsarwar/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip/0_BERT/added_tokens.json\n2020-02-16 10:09:37 - loading file /net/home/smsarwar/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip/0_BERT/special_tokens_map.json\n2020-02-16 10:09:37 - loading file None\n2020-02-16 10:09:38 - Use pytorch device: cuda\nBatches: 100%|██████████| 1/1 [00:00<00:00, 47.35it/s]Sentence: This framework generates embeddings for each input sentence\nEmbedding: 768\n\nSentence: Sentences are passed as a list of string.\nEmbedding: 768\n\nSentence: The quick brown fox jumps over the lazy dog.\nEmbedding: 768\n\n\n"
    }
   ],
   "source": [
    "\"\"\"\n",
    "This basic example loads a pre-trained model from the web and uses it to\n",
    "generate sentence embeddings for a given list of sentences.\n",
    "\"\"\"\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, LoggingHandler\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "#### Just some code to print debug information to stdout\n",
    "np.set_printoptions(threshold=100)\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                    level=logging.INFO,\n",
    "                    handlers=[LoggingHandler()])\n",
    "#### /print debug information to stdout\n",
    "\n",
    "\n",
    "\n",
    "# Load Sentence model (based on BERT) from URL\n",
    "model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "\n",
    "# Embed a list of sentences\n",
    "sentences = ['This framework generates embeddings for each input sentence',\n",
    "             'Sentences are passed as a list of string.',\n",
    "             'The quick brown fox jumps over the lazy dog.']\n",
    "sentence_embeddings = model.encode(sentences)\n",
    "\n",
    "# The result is a list of sentence embeddings as numpy arrays\n",
    "for sentence, embedding in zip(sentences, sentence_embeddings):\n",
    "    print(\"Sentence:\", sentence)\n",
    "    print(\"Embedding:\", len(embedding))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "0\nbegin\n0\nend\n1\n3\n7\n11\nbegin\n3\nend\n14\n7\n12\nbegin\n4\nend\n13\n[['This'], ['that', 'Iraqi', 'Vice', 'President', 'Ramadan', 'had', 'said', 'would', 'take', 'place', '.'], ['Iraqi', 'Vice', 'President', 'Ramadan', 'had', 'said', 'would', 'take', 'place']]\n"
    }
   ],
   "source": [
    "from predpatt import PredPatt\n",
    "\n",
    "def get_event_windows(text):\n",
    "    pp = PredPatt.from_sentence(text)    \n",
    "    #pp = PredPatt.from_sentence('This is something that Iraqi Vice President Ramadan had said would take place.')\n",
    "    tokens = [token.text for token in pp.tokens]\n",
    "    windows = []\n",
    "\n",
    "    for event in pp.events:\n",
    "        #print(event)\n",
    "        #print(event.position)\n",
    "        window_begin = 0\n",
    "        window_end = 0\n",
    "        for i, argument in enumerate(event.arguments):        \n",
    "            print(argument.position)    \n",
    "            if i == 0:\n",
    "                window_begin = argument.position - len(argument.phrase().strip().split()) + 1\n",
    "            if i == len(event.arguments) - 1:\n",
    "                window_end = argument.position + len(argument.phrase().strip().split())              \n",
    "        #windows.append((window_begin, window_end))\n",
    "        print(\"begin\")\n",
    "        print(window_begin)\n",
    "        print(\"end\")\n",
    "        print(window_end)\n",
    "        windows.append(tokens[window_begin: window_end])\n",
    "    \n",
    "    #for window in windows:\n",
    "    return windows\n",
    "\n",
    "print(get_event_windows(\"This is something that Iraqi Vice President Ramadan had said would take place.\"))\n",
    "\n",
    "#print(pp.pprint())\n",
    "# print(\" \".join([token.text for token in pp.tokens]))\n",
    "# print(pp.events)\n",
    "# print(pp.event_dict)\n",
    "# print(pp.events)\n",
    "\n",
    "# for event in pp.events:\n",
    "#     print(event)\n",
    "#     print(event.position)\n",
    "#     for argument in event.arguments:\n",
    "#         print(argument.phrase())\n",
    "#         print(argument.position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}